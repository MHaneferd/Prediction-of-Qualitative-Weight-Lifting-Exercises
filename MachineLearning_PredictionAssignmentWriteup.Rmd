---
title: 'Machine Learning Project: Prediction Assignment Writeup'
author: "Martin Haneferd"
date: "21 jan. 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview
The goal of this project is to predict 20 occurences of correctly and incorrectly barebell lifting excercises. 
There are two datasets (training and testing) that was provided to find the right model for prediction. The dataset's are provided by Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises.
Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4VrFz3TdU

The data collection in the datasets was done by use of data from accelerometers on the belt, forearm, arm, and dumbell. The "classe" variable in the training set is the dependent variable that needs to be predicted in the test dataset at the end.

## Management summary
To be able to predict 20 occurences with 100% accuracy, the model need to be extremly accurate. I was therefore creating a training and test set out of the training dataset.
I used Random forest to create a model 
The final model got an accuracy over 99%. This is good enough to predict the 20 values in test set. And it "thankfully" matched the Quiz-answers 100%.

## Load libraries
```{r, message=FALSE, warning=FALSE}
library(RCurl)
library(caret)
library(randomForest)
library(rfUtilities)
set.seed(1)
```


## Data load
Download and read the datasets.
```{r}
# Get Training dataset
train_file <- "./pml-training.csv"
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
              destfile=train_file, method="libcurl")

# Get test dataset
test_file <- "./pml-testing.csv"
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile=test_file, method="libcurl")

# Read training and test dataset
dfTrain <- read.csv(train_file, na.strings=c("NA",""))
dfTest <- read.csv(test_file, na.strings=c("NA",""))
```

## Data Exploration and Preparation

Since there are some unneccesarry variables, and a huge amount of missing data (NA's), I will remove these columns to be able to compute randomforest.
```{r}
# Look at datasets
dim(dfTrain)
dim(dfTest)
#str(dfTrain) -> Have commented it out since it produced a lot of output.

# Remove the index
dfTrain <- dfTrain[,-1] 

# Remove the columns that have over 70% missing data/values
numcol <- ncol(dfTrain)
numrow <- nrow(dfTrain)
colsWithData <- c( colSums( !is.na( dfTrain[,-numcol] ) ) >= 0.7 * numrow ) # True if data.
dfTrain <- dfTrain[,colsWithData] # Only keep the columns with data

# convert the timestamps from factor to datetime
dfTrain$cvtd_timestamp <- as.POSIXlt(as.character(dfTrain$cvtd_timestamp), format="%d/%m/%Y %H:%M")
dfTest$cvtd_timestamp <- as.POSIXlt(as.character(dfTest$cvtd_timestamp), format="%d/%m/%Y %H:%M")

dim(dfTrain) # See the dimensions of the dataset.
```

I use the random forest utillity package to select the important variables/features to build the model
```{r}
# Find the important features in the datasets.
rf.class <- rf.modelSel(dfTrain[,1:58], dfTrain[,"classe"], ntree = 100)

# importance of selected features
plot(rf.class) 

# Put the important features in a variable.
vars <- rf.class$selvars

length(vars) # Look at the number of variables left

dfTrain <- dfTrain[,c(vars,"classe")]
dfTest <- dfTest[,vars]
```

Split the training dataset into two datasets: training (80%) and testing (20%) 
```{r}
inTrain <- createDataPartition(y = dfTrain$classe, p = .80, list = FALSE)
training <- dfTrain[ inTrain,]
testing <- dfTrain[-inTrain,]

# Check the sizes of the training and test set.
dim(training)
dim(testing)
```


## Perform prediction modelling
I use random forest to fit the model.
```{r}
# Build the Random forest model
rf.mdl <- randomForest(training[,vars], 
                       training[,"classe"], 
                       importance = TRUE)
```

## Analyzing the model
The model is analyzed with cross validation. The default values is 99 folds with 10% data withhold. The kappa and accuracy is then measured of the model. As you will see the kappa will be over 99% for both the model and cross validation of the model. The accuracy related to both sensitivity and specificity by each class is very good. There is no sign of overfitting. 
```{r}
# Do a cross validation on the model and look at Kappa for evaluating the classifiers.
rf.cv <- rf.crossValidation(rf.mdl, training[,vars])
plot(rf.cv, type = "cv", stat = "kappa")

# Investigate the accuracy provided by the cross validation of the model
summary(rf.cv)

# Look at the accuracy of the model on the testing subset of the provided train dataset.
rf.testing.mdl <-  predict(rf.mdl, newdata=testing[,vars])
accuracy(rf.testing.mdl, testing$classe)

# Confusion Matrix by Class only:
t(confusionMatrix(rf.testing.mdl,testing$classe)$byClass)
```

## Do final prediction
```{r}

# Use the model to predict the outcome of the provided test dataset
rf.final <- predict(rf.mdl, dfTest)
rf.final
```

## Summary
The random forest model selection turns out to be extremly accurate. It is fast when you use the rfUtility package to select out the most important variables, and run the random forest with default values. The forecast turned out to be exact and gave 100% on the quiz.



